{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## **Mathematical basis**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Least squares approximation\n",
    "\n",
    "For a given data set that can be represented by a cloud of $m+1 \\in \\mathbb N$ points $(x_i, y_i)_{0 \\le i \\le m}$ in the plane i.e. in $\\mathbb R^2$  often one would like to find a best fit line or in general a polynomial of degree $n$ that best approximates the future development of the data. One way of doing this is via least squares method. This means for all points of our data set we'd like to minimize the distance to the polynomial $a_0+a_1T+...+a_nT^n \\in \\mathbb R[T]$. So we're looking for the vector $(a_0,...a_n) \\in \\mathbb R ^{n+1}$ such that\n",
    "\n",
    "$$\n",
    "(y_0 - (a_0 + a_1x_0 + ... +a_nx_0^n))^2 +...+(y_m - (a_0+a_1x_m+...+a_nx_m^n))^2\n",
    "$$\n",
    "\n",
    "becomes minimal.\n",
    "\n",
    "We can write this in matrix form: Let $Y = \\left(\\begin{array} {} y_0 \\\\ . \\\\. \\\\ . \\\\y_m \\end{array}\\right) \\in \\mathbb R^{m+1}$ , $X = \\left(\\begin{array} {} a_n \\\\ . \\\\. \\\\ . \\\\ a_0 \\end{array}\\right) \\in \\mathbb R^{n+1}$  and $A = \\left(\\begin{array} {} x_0^n&...&x_0& 1 \\\\ .&...&.&. \\\\.&...&.&. \\\\ .&...&.&. \\\\ x _m^n&...&x_m& 1\\end{array}\\right) \\in \\mathbb R^{m+1 \\times n+1}$ . Then we are looking for\n",
    "\n",
    "$$\n",
    "\\min_{X\\in\\mathbb{R}^{n+1}}\\ \\lVert Y-AX \\rVert^2.\n",
    "$$\n",
    "\n",
    "Notice that this is a special case for a minimisation problem of the form $\\min_{X \\in U} \\lVert Y-X \\rVert^2$ for $U$ a subspace of $\\mathbb R ^k$ for some $k \\in \\mathbb N$ and $Y \\in \\mathbb R ^k$. The following theorem tells us what the solution for such problems is.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Theorem 1.** Let $Y \\in \\mathbb R^k$ for some $k \\in \\mathbb N$ and $U$ a subspace of $\\mathbb R ^k$. Let $Y=Y_U + Y_{U^\\perp}$ be the orthogonal decomposition of $Y$ with regards to $U$. Then\n",
    "\n",
    "$$\n",
    "\\lVert Y - Y_U \\rVert  \\leq \\ \\lVert Y-X\\rVert\n",
    "$$\n",
    "\n",
    "for all $X\\in\\ U$. The inequality is an equality if and only if $X=Y_U$.\n",
    "\n",
    "**Proof.** Let $X\\in\\ U$. Then\n",
    "\n",
    "$\\lVert Y - Y_U \\rVert^2  \\leq \\ \\lVert Y - Y_U \\rVert^2 + \\lVert Y_U - X \\rVert^2 = \\lVert Y-Y_U+Y_U-X \\rVert^2 = \\lVert Y-X \\rVert^2$ \n",
    "\n",
    "The first equality follows from the pythagorean theorem because $Y-Y_U = Y_{U ^\\perp} \\in\\ U^\\perp$  and $Y_U - X \\in\\ U$.\n",
    "\n",
    "By taking the square root we obtain $\\lVert Y - Y_U \\rVert  \\leq \\ \\lVert Y-X\\rVert$ .\n",
    "\n",
    "Notice that the inequality is an equality if and only if $\\lVert Y_U - X \\rVert = 0$. This is the case if and only if $Y_U =X$ .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "By Theorem 1 we now know that the solution to our problem is given by $X \\in \\mathbb R^{n+1}$ such that $AX=PY$ where $P$ is the matrix, in the canonical basis of $\\mathbb R^{m+1}$, of the orthogonal projection to $\\mathrm{Im} A$. The following theorem tells us how to compute this projection matrix $P$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Theorem 2.**\n",
    "Let $k \\leqslant n$ and let $A\\in\\mathrm{Mat}(n\\times k;\\mathbb{R})$ be a matrix, which we think of as a family of $ k $ vectors in $ \\mathbb{R}^n$. Let $P$ be the matrix, in the canonical basis of $\\mathbb{R}^n$, of the orthogonal projection to $\\mathrm{Im}\\,A$. If $\\mathrm{rank}(A)=k$, then the matrix $(A^tA)\\in\\mathrm{Mat}(n\\times n;\\mathbb{R})$ is invertible and we have $$P=A(A^tA)^{-1}A^t\\,.$$\n",
    "\n",
    "**Proof.** Notice that $A$ has full rank, therefore we know from linear algebra that the columns of $A$ form a basis of $\\mathrm{Im} A$. In particular $\\mathrm{dim (Im}A) = k$.\n",
    "\n",
    "First we want to prove that $(A^tA)\\in\\mathrm{Mat}(n\\times n;\\mathbb{R})$ is invertible. From linear algebra we know that $A^tA$  is invertible if and only if left multiplication by $A^tA$ is bijective. We start by showing injectivity via $\\mathrm{Ker}A^tA =0$.\n",
    "\n",
    "Let $Y\\in\\ \\mathbb R^n$ with $0=A^tAY$. By multiplication with $Y^t$ we obtain $0=Y^tA^tAY=(AY)^tAY=\\lVert AY\\rVert^2$ . It follows that $AY=0$, therefore $Y\\in\\ \\mathrm{Ker}A$. But by rank\\-nullity\\-theorem we know $\\mathrm{dim (Ker}A) =0$ and it follows that $Y=0$.\n",
    "\n",
    "It is clear that $0 \\in\\ \\mathrm {Ker}A^tA$.\n",
    "\n",
    "It follows in particular that $\\mathrm{dim (Ker}A^tA) = 0$.\n",
    "\n",
    "Then by rank\\-nullity\\-theorem we obtain $\\mathrm{dim (Im}A^tA) = n$, this shows surjectivity.$\\newline$ \n",
    "\n",
    "Now we want to prove $P=A(A^tA)^{-1}A^t$ .\n",
    "\n",
    "Let $Y\\in\\ \\mathbb{R}^k$ and $Y= Y_{\\mathrm{Im}A} + Y_{(\\mathrm{Im}A)^\\perp}$ be the orthogonal decomposition of $Y$ with regards to $\\mathrm{Im} A$. Notice that $Y_{\\mathrm{Im}A} \\in\\ \\mathrm{Im}A$ and therefore there exists a $X \\in\\  \\mathbb R^n$  such that $AX=Y_{\\mathrm{Im}A}$. Then $Y_{(\\mathrm{Im}A)^\\perp} = Y -  Y_{\\mathrm{Im}A} = Y - AX\\in\\ (\\mathrm{Im}A)^\\perp$.\n",
    "\n",
    "By rank\\-nullity\\-theorem we know $\\mathrm{dim (Im}A) + \\mathrm{dim(Im}A)^\\perp = n =\\mathrm{dim(Ker}A^t) + \\mathrm{dim(Im}A^t) = \\mathrm{dim(Ker}A^t) + \\mathrm{dim(Im}A)$.\n",
    "\n",
    "We obtain $\\mathrm{dim(Im}A)^\\perp = \\mathrm{dim(Ker}A^t)$  and it follows that $\\mathrm{(Im}A)^\\perp = \\mathrm{Ker}A^t$.\n",
    "\n",
    "Then $0=A^t(Y-AX) = A^tY -A^tAX$. It follows: $A^tY= A^tAX$.\n",
    "\n",
    "By mulitplication with $(A^tA)^{-1}$ we obtain $(A^tA)^{-1}A^tY=X$.\n",
    "\n",
    "By multiplication with A we obtain $A(A^tA)^{-1}A^tY=AX=Y_{\\mathrm{Im}A}$.\n",
    "\n",
    "It follows that $P=A(A^tA)^{-1}A^t$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Often it is more practical to immediately know the vector $X \\in\\ \\mathbb R ^{n+1}$ that minimizes $\\lVert Y - AX \\rVert$ rather than first calculating the projection matrix $P$. We use the following corollary.\n",
    "\n",
    "**Corollary 3.**\n",
    "Let $k\\leqslant n$ and let $A\\in\\mathrm{Mat}(n\\times k;\\mathbb{R})$ be a matrix. If $\\mathrm{rank}(A)=k$, then for all $Y\\in\\mathbb{R}^n$, the least squares minimisation problem $$\\min_{X\\in\\mathbb{R}^k}\\|Y-AX\\|^2$$ admits the vector $$X=(A^tA)^{-1}A^t Y$$ as its unique solution.\n",
    "\n",
    "**Proof.** We know that $\\lVert Y - AX \\rVert$ is minimal for $AX =Y_{\\mathrm{Im}A} = PY$. We saw in the proof of the theorem that in this situation $X=(A^tA)^{-1}A^tY$ .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Note the special case of Corollary 3 for $k=n$.\n",
    "\n",
    "**Corollary 4.** Let $A\\in\\mathrm{Mat}(n\\times n;\\mathbb{R})$ be a matrix. If $\\mathrm{rank}(A)=n$, then A is invertible and for all $Y\\in\\mathbb{R}^n$, the least squares minimisation problem $$\\min_{X\\in\\mathbb{R}^n}\\|Y-AX\\|^2$$ admits the vector $X=A^{-1}Y$ as its unique solution.\n",
    "\n",
    "**Proof.** Note that A has full rank and is therefore invertible. We know from Corollary 3 that $X=(A^tA)^{-1}A^tY =A^{-1}(A^t)^{-1}A^tY = A^{-1}Y$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Lagrange interpolating polynomial\n",
    "\n",
    "Instead of approximating we can also find a polynomial $L(x) \\in \\mathbb R[x]$ of $\\mathrm {deg}L \\leq m$ that passes through each data point in $(x_i, y_i)_{0 \\le i \\le m}$, so $L(x_i) = y_i$ for each $x_i$. We call $L(x)$ the Lagrange interpolating polynomial. Let $L(x) = \\sum_{i = 0}^{m} a_i x^{i} \\in \\mathbb R[x]$ in standard basis. In order to find the coefficients $(a_i)_{0 \\le i \\le m}$ of $L$ we'd like to solve the following system of linear equations:\n",
    "\n",
    "$$\n",
    "\\sum _{i=0}^{m}a_ix_k^i=y_k,0≤k≤m\n",
    "$$\n",
    "\n",
    "We can write this in matrix form as well: Let $Y = \\left(\\begin{array} {} y_0 \\\\ . \\\\. \\\\ . \\\\y_m \\end{array}\\right) \\in \\mathbb R^{m+1}$ , $X = \\left(\\begin{array} {} a_m \\\\ . \\\\. \\\\ . \\\\ a_0 \\end{array}\\right) \\in \\mathbb R^{m+1}$  and $A = \\left(\\begin{array} {} x_0^m&...&x_0& 1 \\\\ .&...&.&. \\\\.&...&.&. \\\\ .&...&.&. \\\\ x _m^m&...&x_m& 1\\end{array}\\right) \\in \\mathbb R^{m+1 \\times m+1}$.\n",
    "\n",
    "Then $AX=Y$.\n",
    "\n",
    "Note that this is exactly the situation from Corollary 4. Therefore it follows: If we use least squares method to approximate by a function of $\\mathrm{deg} = m$ the polynomial we obtain coincides with the Lagrange polynomial.\n",
    "\n",
    "The following theorem tells us how to calculate the Lagrange polynomial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Theorem 5.** For a set of points $(x_i, y_i)_{0 \\le i \\le n} \\subseteq \\mathbb R^2$ with $x_i \\neq x_j$ for $i \\neq j$ let $L(x) \\in \\mathbb R[x]$, $\\mathrm{deg}L = n$, be defined as\n",
    "\n",
    "$$\n",
    "L(x) = \\sum_{i = 0}^{n} y_i l_i(x)\n",
    "$$\n",
    "\n",
    "for $l_i(x) = \\prod_{0\\le j \\le n, j \\neq i} \\frac {x-x_j} {x_i-x_j}$ . Then $L(x)$ is unique and fulfills $L(x_i) = y_i$ for each $i$.\n",
    "\n",
    "**Proof.** Define the $l_i(x)$ as above. Notice that $l_i(x_k) = 0$ for $k\\neq i$ and $l_i(x_i) = 1$.\n",
    "\n",
    "We show that $\\{l_i(x) \\}_{0 \\le i \\le n}$ form a generating set of $V$ the vector space of polynomials of degree less than or equal to $n$ over $\\mathbb R$. We know from linear algebra that $\\mathrm{dim}V = n +1$. Let $f(x)  \\in V$. Then define $q(x) := f(x) - \\sum_{i = 0}^{n} f(x_i) l_i(x)$. Observe that  $\\mathrm{deg} l_i = n$ for each $i$ and $\\mathrm {deg}f \\le n$ and therefore $\\mathrm {deg} q \\le n$. Notice that $q(x_i) = 0$ for each $i$. Then $q$ has $n+1$ roots and must therefore be the zero polynomial. It follows that $f(x) = \\sum_{i = 0}^{n} f(x_i) l_i(x)$. By dimensional reasons $\\{l_i(x) \\}_{0 \\le i \\le n}$ form a basis of $V$.\n",
    "\n",
    "In particular we have shown that $\\sum_{i = 0}^{n} y_i l_i(x) = \\sum_{i = 0}^{n} L(x_i) l_i(x)$ is a unique representation of $L(x)$.\n",
    "\n",
    "Note that $L(x)$ is unique: For any $P(x) \\in V$ a polynomial with $P(x_i) = y_i$ for each $i$ it follows by the above $P(x) = \\sum_{i = 0}^{n} P(x_i) l_i(x)=\\sum_{i = 0}^{n} y_i l_i(x) = \\sum_{i = 0}^{n} L(x_i) l_i(x) = L(x)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Coding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def bestFitCoeffMatrix(data, polyDegree):\n",
    "    # Construction of matrix A\n",
    "    A = matrix(QQ, len(data), polyDegree + 1)\n",
    "    for i in range(len(data)):\n",
    "        for j in range(polyDegree + 1):\n",
    "            A[i, j] = data[i][0] ** (polyDegree - j) \n",
    "    \n",
    "    # Construction of matrix Y\n",
    "    Y = matrix(QQ, len(data), 1, [d[1] for d in data])\n",
    "    \n",
    "    # Calculation of matrix M\n",
    "    M = (A.transpose() * A).inverse() * A.transpose()\n",
    "    \n",
    "    # Calculation of matrix X\n",
    "    X = M * Y\n",
    "    X = vector(X)\n",
    "    return X\n",
    "    \n",
    "\n",
    "\n",
    "def bestFitPoly(X):\n",
    "    #representaion of a polynomial given the coefficient matrix\n",
    "    x = var('x')\n",
    "    g = 0\n",
    "    for i in range(len(X)):\n",
    "        g += X[i] * x ** (len(X) - i - 1)\n",
    "    return g\n",
    "\n",
    "def graph(data, PolyDeg):\n",
    "    B = bestFitCoeffMatrix(data, PolyDeg)\n",
    "    g = bestFitPoly(B)\n",
    "    \n",
    "    G = plot([])\n",
    "    G += list_plot(data, size = 20, color = 'red', \n",
    "    xmin = 0, xmax = 8, ymin = -5, ymax = 80)\n",
    "    G += g.plot(xmin=0,xmax=8,ymin=-5,ymax=80)\n",
    "    return G\n",
    "    \n",
    "\n",
    "\n",
    "def userInterfaceBestFit():\n",
    "    data_lenght = input(\"how many points do you want to add?\")\n",
    "\n",
    "    \n",
    "    # Convert the input to an integer\n",
    "    try:\n",
    "        data_lenght = int(data_lenght)\n",
    "        \n",
    "    except ValueError:\n",
    "        print(\"Please enter a valid integer.\")\n",
    "\n",
    "    #feed the data\n",
    "    data = list()\n",
    "    for i in range(data_lenght):\n",
    "        newData = input(\"give a new data: \")\n",
    "        try:\n",
    "            newData = int(newData)\n",
    "        \n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid integer.\")\n",
    "            \n",
    "        data.append([i + 1, newData])\n",
    "\n",
    "    #create the plot for data\n",
    "    G = plot([])\n",
    "    G += list_plot(data, size = 20, color = 'red', \n",
    "    xmin = 0, xmax = 8, ymin = -5, ymax = 80)\n",
    "    show(G)\n",
    "\n",
    "    #write down the degree\n",
    "    degree = input(\"which degree of approximation you would like?: \")\n",
    "\n",
    "    try:\n",
    "        degree = int(degree)\n",
    "        \n",
    "    except ValueError:\n",
    "        print(\"Please enter a valid integer.\")\n",
    "\n",
    "    if (degree >= data_lenght): #every poly of degree greater than len(data) - 1 will be the same lagrange poly. Therefore no higher degree.\n",
    "        print(\"you passed beyond Lagrange!!\")\n",
    "\n",
    "    else:\n",
    "        B = bestFitCoeffMatrix(data, degree)\n",
    "        P = bestFitPoly(B)\n",
    "        show(P)\n",
    "        show(graph(data, degree))\n",
    "\n",
    "    question = input(\"would you like to see how bad were lagrange? Y/N: \")\n",
    "\n",
    "    if (question == 'Y'):\n",
    "        show(graph(data, len(data) - 1))\n",
    "\n",
    "    else:\n",
    "        print(\"Tchüss!\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "userInterfaceBestFit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "sage-9.8",
    "--python",
    "-m",
    "sage.repl.ipython_kernel",
    "--matplotlib=inline",
    "-f",
    "{connection_file}"
   ],
   "display_name": "SageMath 9.8",
   "env": {
   },
   "language": "sagemath",
   "metadata": {
    "cocalc": {
     "description": "Open-source mathematical software system",
     "priority": 1,
     "url": "https://www.sagemath.org/"
    }
   },
   "name": "sage-9.8",
   "resource_dir": "/ext/jupyter/kernels/sage-9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}